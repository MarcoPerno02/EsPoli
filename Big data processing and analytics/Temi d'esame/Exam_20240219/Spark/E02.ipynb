{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiamo le librerie necessarie\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import col, year, sum, to_timestamp, count, expr, max, count_distinct\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "import numpy as np\n",
    "\n",
    "# Supponiamo che SparkSession sia gi√† stato creato\n",
    "ss = SparkSession.builder.appName(\"PoliSalesAnalysis\").getOrCreate()\n",
    "\n",
    "# Variabili per i percorsi di input e output\n",
    "# Percorsi dei file di input e output\n",
    "jupyter = False\n",
    "if jupyter:\n",
    "    input_prefix = \"/user/s339450/esami/20240912/\"\n",
    "    output_prefix= \"/user/s339450/esami/20240912/out/\"\n",
    "else:\n",
    "    input_prefix = \".\\\\data\\\\\"\n",
    "    output_prefix= \".\\\\out\\\\\"\n",
    "\n",
    "catalouges_path = f\"{input_prefix}Catalouges.txt\"\n",
    "users_path = f\"{input_prefix}Users.txt\"\n",
    "purchases_path = f\"{input_prefix}Purchases.txt\"\n",
    "output_folder_1 = f\"{output_prefix}1/\"\n",
    "output_folder_2 = f\"{output_prefix}2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------------+-----------------+\n",
      "| ItemID|        Name|      Category|StillinProduction|\n",
      "+-------+------------+--------------+-----------------+\n",
      "|Item123| SmartphoneX|   Electronics|             True|\n",
      "|Item234|     LaptopY|   Electronics|            False|\n",
      "|Item345|    BlenderZ|HomeAppliances|             True|\n",
      "|Item456| T-ShirtBlue|      Clothing|             True|\n",
      "|Item567|RunningShoes|      Clothing|            False|\n",
      "|Item678|     BookABC|         Books|             True|\n",
      "|Item789| GameConsole|   Electronics|             True|\n",
      "|Item890|  SmartWatch|   Electronics|             True|\n",
      "+-------+------------+--------------+-----------------+\n",
      "\n",
      "+------+-------+--------+-----------+--------+\n",
      "|UserID|   Name| Surname|       City| Country|\n",
      "+------+-------+--------+-----------+--------+\n",
      "|User01|   John|     Doe|   New York|     USA|\n",
      "|User02|   Jane|   Smith|     London|      UK|\n",
      "|User03|  Paolo|   Garza|      Turin|   Italy|\n",
      "|User04|    Mei|      Li|    Beijing|   China|\n",
      "|User05|  Aisha|    Khan|     Lahore|Pakistan|\n",
      "|User06|  Maria|Gonzalez|Mexico City|  Mexico|\n",
      "|User07|   Kofi|  Mensah|      Accra|   Ghana|\n",
      "|User08|   Anna| Ivanova|     Moscow|  Russia|\n",
      "|User09|Satoshi|Yamamoto|      Tokyo|   Japan|\n",
      "|User10| Carlos|   Silva|  Sao Paulo|  Brazil|\n",
      "+------+-------+--------+-----------+--------+\n",
      "\n",
      "+-------------------+------+-------+-------+\n",
      "|      SaleTimestamp|UserID| ItemID|Country|\n",
      "+-------------------+------+-------+-------+\n",
      "|2020-01-15 14:23:45|User01|Item123|  29.99|\n",
      "|2020-03-10 08:15:30|User02|Item234|   45.0|\n",
      "|2020-06-25 18:00:00|User01|Item345|   10.5|\n",
      "|2021-02-12 10:05:20|User03|Item123|  29.99|\n",
      "|2022-07-20 11:30:00|User04|Item456|  89.99|\n",
      "|2022-07-21 11:30:00|User04|Item456|  89.99|\n",
      "|2022-07-22 11:30:00|User04|Item456|  89.99|\n",
      "|2022-07-22 11:30:00|User01|Item456|  89.99|\n",
      "|2022-11-05 15:45:30|User02|Item567|   15.0|\n",
      "|2022-11-06 15:45:30|User02|Item567|   15.0|\n",
      "|2022-11-07 15:45:30|User02|Item567|   15.0|\n",
      "|2023-01-08 12:50:10|User05|Item678|   50.0|\n",
      "|2023-01-08 12:50:11|User05|Item678|   60.0|\n",
      "|2023-01-08 12:50:10|User03|Item678|   50.0|\n",
      "|2023-05-14 09:10:15|User03|Item789|  120.0|\n",
      "|2023-10-01 20:40:55|User01|Item890|   30.0|\n",
      "+-------------------+------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "catalouges_schema = StructType([\n",
    "    StructField(\"ItemID\", StringType(), False),\n",
    "    StructField(\"Name\", StringType(), False),\n",
    "    StructField(\"Category\", StringType(), False),\n",
    "    StructField(\"StillinProduction\", StringType(), False),\n",
    "])\n",
    "\n",
    "catalouges: DataFrame = ss.read.load(catalouges_path,\n",
    "    format=\"csv\",\n",
    "    header=False,\n",
    "    schema=catalouges_schema)\n",
    "\n",
    "catalouges.show()\n",
    "\n",
    "users_schema = StructType([\n",
    "    StructField(\"UserID\", StringType(), False),\n",
    "    StructField(\"Name\", StringType(), False),\n",
    "    StructField(\"Surname\", StringType(), False),\n",
    "    StructField(\"City\", StringType(), False),\n",
    "    StructField(\"Country\", StringType(), False)\n",
    "])\n",
    "\n",
    "users: DataFrame = ss.read.load(users_path,\n",
    "    format=\"csv\",\n",
    "    header=False,\n",
    "    schema=users_schema)\n",
    "\n",
    "users.show()\n",
    "\n",
    "purchases_schema = StructType([\n",
    "    StructField(\"SaleTimestamp\", StringType(), False),\n",
    "    StructField(\"UserID\", StringType(), False),\n",
    "    StructField(\"ItemID\", StringType(), False),\n",
    "    StructField(\"Country\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "purchases: DataFrame = ss.read.load(purchases_path,\n",
    "    format=\"csv\",\n",
    "    header=False,\n",
    "    schema=purchases_schema)\n",
    "\n",
    "# Conversione del timestamp\n",
    "purchases = purchases.withColumn(\n",
    "    \"SaleTimestamp\",\n",
    "    to_timestamp(col(\"SaleTimestamp\"), \"yyyy/MM/dd-HH:mm:ss\")  # Adatta il formato del timestamp\n",
    ")\n",
    "\n",
    "purchases.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|UserID|\n",
      "+------+\n",
      "|User04|\n",
      "|User02|\n",
      "|User03|\n",
      "|User05|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Aggregate purchases by UserID and Year\n",
    "response1 = (\n",
    "    purchases.filter((year(col(\"SaleTimestamp\")) == 2022) | (year(col(\"SaleTimestamp\")) == 2023))\n",
    "    .groupBy(col(\"UserID\"), year(col(\"SaleTimestamp\")).alias(\"Year\"))\n",
    "    .agg(count(expr(\"*\")).alias(\"TotalPurchases\"))\n",
    ")\n",
    "\n",
    "# Step 2: Define a window specification for each year\n",
    "window_spec = Window.partitionBy(\"Year\")\n",
    "\n",
    "# Step 3: Add a column for the maximum purchases for each year\n",
    "response1 = response1.withColumn(\"MaxPurchaseForEachYear\", max(\"TotalPurchases\").over(window_spec))\n",
    "\n",
    "response1 = response1.filter(col(\"TotalPurchases\") == col(\"MaxPurchaseForEachYear\"))\n",
    "\n",
    "response1 = response1.select(\"UserID\")\n",
    "\n",
    "# Show the result\n",
    "response1.show()\n",
    "\n",
    "response1.write.mode(\"overwrite\").csv(output_folder_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|   Category| ItemID|\n",
      "+-----------+-------+\n",
      "|   Clothing|Item456|\n",
      "|      Books|Item678|\n",
      "|Electronics|Item890|\n",
      "|Electronics|Item789|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(\"Year\", \"Category\")\n",
    "\n",
    "response2 = (\n",
    "    purchases.join(catalouges, \"ItemID\")\n",
    "    .filter((year(col(\"SaleTimestamp\")) == 2022) | (year(col(\"SaleTimestamp\")) == 2023))\n",
    "    .groupBy(col(\"ItemID\"), col(\"Category\"), year(col(\"SaleTimestamp\")).alias(\"Year\"))\n",
    "    .agg(count_distinct(col(\"UserID\")).alias(\"DistinctUserIdForYear\"))\n",
    ")\n",
    "\n",
    "response2 = response2.withColumn(\"MaxDistinctUserIDForEachYear\", max(col(\"DistinctUserIdForYear\")).over(window_spec))\n",
    "\n",
    "response2 = response2.filter(col(\"DistinctUserIdForYear\") == col(\"MaxDistinctUserIDForEachYear\"))\n",
    "\n",
    "response2 = response2.select(\"Category\", \"ItemID\")\n",
    "\n",
    "response2.show()\n",
    "\n",
    "response2.write.mode(\"overwrite\").csv(output_folder_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
