{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiamo le librerie necessarie\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "from pyspark.sql.functions import col, year, sum\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "import numpy as np\n",
    "\n",
    "# Supponiamo che SparkSession sia gi√† stato creato\n",
    "ss = SparkSession.builder.appName(\"PoliSalesAnalysis\").getOrCreate()\n",
    "\n",
    "# Variabili per i percorsi di input e output\n",
    "# Percorsi dei file di input e output\n",
    "jupyter = False\n",
    "if jupyter:\n",
    "    input_prefix = \"/user/s339450/esami/20240912/\"\n",
    "    output_prefix= \"/user/s339450/esami/20240912/out/\"\n",
    "else:\n",
    "    input_prefix = \".\\\\data\\\\\"\n",
    "    output_prefix= \".\\\\out\\\\\"\n",
    "\n",
    "job_contracts_path = f\"{input_prefix}JobContracts.txt\"\n",
    "job_offers_path = f\"{input_prefix}JobOffers.txt\"\n",
    "job_postings_path = f\"{input_prefix}JobPostings.txt\"\n",
    "output_folder_1 = f\"{output_prefix}1/\"\n",
    "output_folder_2 = f\"{output_prefix}2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------+------------+\n",
      "|ContractID|OfferId|ContractDate|ContractType|\n",
      "+----------+-------+------------+------------+\n",
      "|       201|    101|  2023-01-15|   Full-time|\n",
      "|       202|    103|  2023-02-01|   Part-time|\n",
      "|       203|    114|  2023-03-10|   Full-time|\n",
      "|       204|    115|  2023-04-20|  Internship|\n",
      "|       205|    116|  2023-05-05|   Full-time|\n",
      "|       206|    117|  2023-06-15|  Contractor|\n",
      "+----------+-------+------------+------------+\n",
      "\n",
      "+-------+-----+------+--------+-----------+\n",
      "|OfferID|JobId|Salary|  Status|        SSN|\n",
      "+-------+-----+------+--------+-----------+\n",
      "|    101|    1| 97000|Accepted|800-11-2222|\n",
      "|    102|    2|120000|Rejected|800-11-2223|\n",
      "|    103|    3|120000|Accepted|800-12-2222|\n",
      "|    104|    5|120000|Rejected|801-21-3222|\n",
      "|    105|    5|120000|Rejected|800-41-2232|\n",
      "|    106|    4|120000|Rejected|800-14-2422|\n",
      "|    107|    3|120000|Rejected|800-17-2252|\n",
      "|    114|    5|120000|Accepted|800-51-2222|\n",
      "|    115|    1|120000|Accepted|800-51-2622|\n",
      "|    116|    5|120000|Accepted|800-14-2262|\n",
      "|    117|    5|120000|Accepted|800-14-2262|\n",
      "+-------+-----+------+--------+-----------+\n",
      "\n",
      "+-----+-----------------+-------+\n",
      "|JobID|            Title|Country|\n",
      "+-----+-----------------+-------+\n",
      "|    1|Software Engineer|     IT|\n",
      "|    2|   Data Scientist|     FR|\n",
      "|    3|Software Engineer|     ES|\n",
      "|    4|Software Engineer|     IT|\n",
      "|    5|   Data Scientist|     IT|\n",
      "+-----+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job_contracts_schema = StructType([\n",
    "    StructField(\"ContractID\", IntegerType(), False),\n",
    "    StructField(\"OfferId\", IntegerType(), False),\n",
    "    StructField(\"ContractDate\", DateType(), False),\n",
    "    StructField(\"ContractType\", StringType(), False),\n",
    "])\n",
    "\n",
    "job_contracts: DataFrame = ss.read.load(job_contracts_path,\n",
    "    format=\"csv\",\n",
    "    header=False,\n",
    "    schema=job_contracts_schema)\n",
    "\n",
    "job_contracts.show()\n",
    "\n",
    "job_offers_schema = StructType([\n",
    "    StructField(\"OfferID\", IntegerType(), False),\n",
    "    StructField(\"JobId\", IntegerType(), False),\n",
    "    StructField(\"Salary\", IntegerType(), False),\n",
    "    StructField(\"Status\", StringType(), False),\n",
    "    StructField(\"SSN\", StringType(), False)\n",
    "])\n",
    "\n",
    "job_offers: DataFrame = ss.read.load(job_offers_path,\n",
    "    format=\"csv\",\n",
    "    header=False,\n",
    "    schema=job_offers_schema)\n",
    "\n",
    "job_offers.show()\n",
    "\n",
    "job_postings_schema = StructType([\n",
    "    StructField(\"JobID\", IntegerType(), False),\n",
    "    StructField(\"Title\", StringType(), False),\n",
    "    StructField(\"Country\", StringType(), False)\n",
    "])\n",
    "\n",
    "job_postings: DataFrame = ss.read.load(job_postings_path,\n",
    "    format=\"csv\",\n",
    "    header=False,\n",
    "    schema=job_postings_schema)\n",
    "\n",
    "job_postings.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punto 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|OfferId|AverageSalary|\n",
      "+-------+-------------+\n",
      "|    115|     120000.0|\n",
      "|    103|     120000.0|\n",
      "|    117|     120000.0|\n",
      "+-------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, asc, avg, count, expr, max\n",
    "\n",
    "result1 = (\n",
    "    job_offers\n",
    "    .filter(col(\"Status\") == \"Accepted\")\n",
    "    .join(job_postings, \"JobId\")\n",
    "    .groupBy(col(\"OfferId\"))\n",
    "    .agg(avg(\"Salary\").alias(\"AverageSalary\"))  # Assegna un alias a avg(Salary)\n",
    "    .sort(desc(\"AverageSalary\"))  # Ordina in base alla colonna con l'alias\n",
    ")\n",
    "\n",
    "result1.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punto 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+--------+\n",
      "|            Title|Country|count(1)|\n",
      "+-----------------+-------+--------+\n",
      "|Software Engineer|     IT|       2|\n",
      "|Software Engineer|     ES|       1|\n",
      "|   Data Scientist|     IT|       3|\n",
      "+-----------------+-------+--------+\n",
      "\n",
      "+-----------------+-------+--------+---------+\n",
      "|            Title|Country|count(1)|max_count|\n",
      "+-----------------+-------+--------+---------+\n",
      "|Software Engineer|     ES|       1|        1|\n",
      "|Software Engineer|     IT|       2|        3|\n",
      "|   Data Scientist|     IT|       3|        3|\n",
      "+-----------------+-------+--------+---------+\n",
      "\n",
      "+-----------------+-------+--------+---------+\n",
      "|            Title|Country|count(1)|max_count|\n",
      "+-----------------+-------+--------+---------+\n",
      "|Software Engineer|     ES|       1|        1|\n",
      "|   Data Scientist|     IT|       3|        3|\n",
      "+-----------------+-------+--------+---------+\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "tmp = (\n",
    "    job_offers\n",
    "    .filter(col(\"Status\") == \"Accepted\")\n",
    "    .join(job_postings, \"JobId\")\n",
    "    .groupBy(col(\"Title\"), col(\"Country\"))\n",
    "    .agg(count(expr(\"*\")))\n",
    "    \n",
    ")\n",
    "tmp.show()\n",
    "\n",
    "tmp_with_max = tmp.withColumn(\"max_count\", max(\"count(1)\").over(Window.partitionBy(\"Country\")))\n",
    "tmp_with_max.show()\n",
    "\n",
    "result = tmp_with_max.filter(col(\"max_count\") == col(\"count(1)\"))\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
